{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Benchmarking Five Transformer Model (BERT, DISTILBERT, ROBERTA, XLNET and ALBERT) For Question Classification**"
      ],
      "metadata": {
        "id": "m5rN2WJTyFMB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "525x0w5mxjFI"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import random\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import joblib\n",
        "\n",
        "from collections import defaultdict\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import classification_report, accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from transformers import (\n",
        "    BertTokenizer, BertForSequenceClassification,\n",
        "    DistilBertTokenizer, DistilBertForSequenceClassification,\n",
        "    RobertaTokenizer, RobertaForSequenceClassification,\n",
        "    XLNetTokenizer, XLNetForSequenceClassification,\n",
        "    AlbertTokenizer, AlbertForSequenceClassification,\n",
        ")\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Reproducibility\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "torch.cuda.manual_seed_all(42)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"questions-data-new.csv\")\n",
        "df = df.rename(columns={\"topic\": \"label\"})\n",
        "\n",
        "# Encode labels\n",
        "le = LabelEncoder()\n",
        "df[\"label\"] = le.fit_transform(df[\"label\"])\n",
        "label_names = le.classes_\n",
        "\n",
        "# Tokenization Helper\n",
        "def tokenize_data(tokenizer, texts, labels, max_length=128):\n",
        "    encodings = tokenizer(texts.tolist(), truncation=True, padding=True, max_length=max_length)\n",
        "    encodings = {key: torch.tensor(val) for key, val in encodings.items()}\n",
        "    labels = torch.tensor(labels.tolist())\n",
        "    dataset = torch.utils.data.TensorDataset(encodings['input_ids'], encodings['attention_mask'], labels)\n",
        "    return dataset\n",
        "\n",
        "# Training Function\n",
        "def train_one_fold(model, train_loader, val_loader, optimizer, device, num_labels, num_epochs, model_name, fold, fold_losses):\n",
        "    model.to(device)\n",
        "    model.train()\n",
        "\n",
        "    all_labels = []\n",
        "    for _, _, labels in train_loader:\n",
        "        all_labels.extend(labels.tolist())\n",
        "\n",
        "    class_weights = compute_class_weight(class_weight='balanced', classes=np.arange(num_labels), y=all_labels)\n",
        "    class_weights = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
        "    loss_fn = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_train_loss = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        for step, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch+1}\")):\n",
        "            input_ids, attention_mask, labels = [b.to(device) for b in batch]\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask=attention_mask)\n",
        "            loss = loss_fn(outputs.logits, labels)\n",
        "            total_train_loss += loss.item()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if step % 25 == 0:\n",
        "                tqdm.write(f\"Epoch {epoch+1}, Step {step}, Train Loss: {loss.item():.4f}\")\n",
        "\n",
        "        avg_train_loss = total_train_loss / len(train_loader)\n",
        "        train_losses.append(avg_train_loss)\n",
        "\n",
        "        model.eval()\n",
        "        total_val_loss = 0\n",
        "        with torch.no_grad():\n",
        "            for val_batch in val_loader:\n",
        "                input_ids, attention_mask, labels = [b.to(device) for b in val_batch]\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                loss = loss_fn(outputs.logits, labels)\n",
        "                total_val_loss += loss.item()\n",
        "\n",
        "        avg_val_loss = total_val_loss / len(val_loader)\n",
        "        val_losses.append(avg_val_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1} - Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}\")\n",
        "\n",
        "    fold_losses[model_name] = {\"train\": train_losses, \"val\": val_losses}\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Plotting losses across folds in a single graph per fold\n",
        "def plot_fold_wise_loss(fold_losses_all_models, num_epochs, fold):\n",
        "    plt.figure(figsize=(12, 6))\n",
        "    colors = sns.color_palette(\"husl\", n_colors=len(fold_losses_all_models))\n",
        "\n",
        "    for idx, (model_name, losses) in enumerate(fold_losses_all_models.items()):\n",
        "        plt.plot(range(1, num_epochs + 1), losses['train'], linestyle='-', label=f\"{model_name} - Train\", color=colors[idx])\n",
        "        plt.plot(range(1, num_epochs + 1), losses['val'], linestyle='--', label=f\"{model_name} - Val\", color=colors[idx])\n",
        "\n",
        "    plt.title(f\"Fold {fold+1} - Training & Validation Loss for All Models\")\n",
        "    plt.xlabel(\"Epoch\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend(loc='upper right', bbox_to_anchor=(1.25, 1))\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    os.makedirs(\"loss_plots_folds\", exist_ok=True)\n",
        "    plt.savefig(f\"loss_plots_folds/fold_{fold+1}_loss_comparison.png\")\n",
        "    plt.close()\n",
        "\n",
        "# Cross-Validation Trainer\n",
        "def train_with_cv(model_name, tokenizer_class, model_class, hyperparams, all_losses, global_fold_losses, fold):\n",
        "    tokenizer = tokenizer_class.from_pretrained(model_name, use_fast=True)\n",
        "    dataset = tokenize_data(tokenizer, df['question'], df['label'])\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    all_metrics = []\n",
        "\n",
        "    classwise_all = defaultdict(lambda: defaultdict(list))  # <== fixed here\n",
        "\n",
        "    for f_idx, (train_idx, val_idx) in enumerate(skf.split(df['question'], df['label'])):\n",
        "        if f_idx != fold:\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n----- Fold {f_idx+1} - Model: {model_name} -----\")\n",
        "\n",
        "        train_subset = Subset(dataset, train_idx)\n",
        "        val_subset = Subset(dataset, val_idx)\n",
        "\n",
        "        train_loader = DataLoader(train_subset, batch_size=hyperparams['batch_size'], shuffle=True)\n",
        "        val_loader = DataLoader(val_subset, batch_size=hyperparams['batch_size'])\n",
        "\n",
        "        model = model_class.from_pretrained(model_name, num_labels=len(label_names))\n",
        "        optimizer = AdamW(model.parameters(), lr=hyperparams['learning_rate'])\n",
        "\n",
        "        fold_losses = {}\n",
        "        train_losses, val_losses = train_one_fold(model, train_loader, val_loader, optimizer, device,\n",
        "                                                  num_labels=len(label_names),\n",
        "                                                  num_epochs=hyperparams.get('epochs', 5),\n",
        "                                                  model_name=model_name,\n",
        "                                                  fold=f_idx,\n",
        "                                                  fold_losses=fold_losses)\n",
        "        global_fold_losses[model_name] = fold_losses[model_name]\n",
        "\n",
        "        model.eval()\n",
        "        val_labels = []\n",
        "        val_preds = []\n",
        "        with torch.no_grad():\n",
        "            for input_ids, attention_mask, labels in val_loader:\n",
        "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                logits = outputs.logits\n",
        "                preds = torch.argmax(logits, dim=1)\n",
        "\n",
        "                val_labels.extend(labels.cpu().numpy())\n",
        "                val_preds.extend(preds.cpu().numpy())\n",
        "\n",
        "        accuracy = accuracy_score(val_labels, val_preds)\n",
        "        precision = precision_score(val_labels, val_preds, average='weighted')\n",
        "        recall = recall_score(val_labels, val_preds, average='weighted')\n",
        "        f1 = f1_score(val_labels, val_preds, average='weighted')\n",
        "\n",
        "        print(\"Classification Report:\")\n",
        "        report = classification_report(val_labels, val_preds, target_names=label_names, output_dict=True)\n",
        "        print(classification_report(val_labels, val_preds, target_names=label_names))\n",
        "\n",
        "        for cls in label_names:\n",
        "            classwise_all[cls][\"precision\"].append(report[cls][\"precision\"])\n",
        "            classwise_all[cls][\"recall\"].append(report[cls][\"recall\"])\n",
        "            classwise_all[cls][\"f1\"].append(report[cls][\"f1-score\"])\n",
        "            classwise_all[cls][\"support\"].append(report[cls][\"support\"])\n",
        "\n",
        "            # Optional: Accuracy per class (manual)\n",
        "            cls_idx = list(label_names).index(cls)\n",
        "            cls_correct = sum((np.array(val_labels) == cls_idx) & (np.array(val_preds) == cls_idx))\n",
        "            cls_total = sum(np.array(val_labels) == cls_idx)\n",
        "            cls_acc = cls_correct / cls_total if cls_total > 0 else 0.0\n",
        "            classwise_all[cls][\"accuracy\"].append(cls_acc)\n",
        "\n",
        "        all_metrics.append({\n",
        "            \"fold\": f_idx + 1,\n",
        "            \"accuracy\": accuracy,\n",
        "            \"precision\": precision,\n",
        "            \"recall\": recall,\n",
        "            \"f1\": f1\n",
        "        })\n",
        "\n",
        "        os.makedirs(\"models\", exist_ok=True)\n",
        "        model_path = f\"models/{model_name.replace('/', '_')}_fold{f_idx+1}.pt\"\n",
        "        torch.save(model.state_dict(), model_path)\n",
        "\n",
        "    results_df = pd.DataFrame(all_metrics)\n",
        "    return results_df, classwise_all\n",
        "\n",
        "\n",
        "# Train all models for each fold and generate single loss plot per fold\n",
        "def train_all_models_foldwise():\n",
        "    model_configs = [\n",
        "        (\"bert-base-uncased\", BertTokenizer, BertForSequenceClassification, {\"batch_size\": 16, \"learning_rate\": 5e-5, \"epochs\": 5}),\n",
        "        (\"distilbert-base-uncased\", DistilBertTokenizer, DistilBertForSequenceClassification, {\"batch_size\": 16, \"learning_rate\": 5e-5, \"epochs\": 5}),\n",
        "        (\"roberta-base\", RobertaTokenizer, RobertaForSequenceClassification, {\"batch_size\": 16, \"learning_rate\": 2e-5, \"epochs\": 5}),\n",
        "        (\"xlnet-base-cased\", XLNetTokenizer, XLNetForSequenceClassification, {\"batch_size\": 16, \"learning_rate\": 2e-5, \"epochs\": 5}),\n",
        "        (\"albert-base-v2\", AlbertTokenizer, AlbertForSequenceClassification, {\"batch_size\": 16, \"learning_rate\": 5e-5, \"epochs\": 5})\n",
        "    ]\n",
        "\n",
        "    all_results = {}\n",
        "    classwise_scores = defaultdict(lambda: defaultdict(lambda: defaultdict(list)))\n",
        "\n",
        "    for fold in range(5):\n",
        "        print(f\"\\n================= Fold {fold+1} =================\")\n",
        "        fold_losses_all_models = {}\n",
        "\n",
        "        for model_name, tokenizer_class, model_class, hyperparams in model_configs:\n",
        "            results_df, classwise_all = train_with_cv(model_name, tokenizer_class, model_class, hyperparams, None, fold_losses_all_models, fold)\n",
        "\n",
        "            if model_name not in all_results:\n",
        "                all_results[model_name] = []\n",
        "\n",
        "            all_results[model_name].append(results_df)\n",
        "\n",
        "            # Save all classwise metrics per fold\n",
        "            for cls in classwise_all:\n",
        "                for metric in classwise_all[cls]:\n",
        "                    classwise_scores[model_name][cls][metric].extend(classwise_all[cls][metric])\n",
        "\n",
        "        plot_fold_wise_loss(fold_losses_all_models, hyperparams.get(\"epochs\", 5), fold)\n",
        "\n",
        "    # Combine and print model-wise average performance\n",
        "    print(\"\\n==== Overall Model-Wise Performance (Across Folds) ====\")\n",
        "    for model_name, all_folds_df in all_results.items():\n",
        "        full_df = pd.concat(all_folds_df)\n",
        "        avg_metrics = full_df[[\"accuracy\", \"precision\", \"recall\", \"f1\"]].mean()\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        print(avg_metrics.to_string(float_format=\"%.4f\"))\n",
        "\n",
        "    # Print class-wise performance (averaged across folds)\n",
        "    print(\"\\n==== Overall Classwise Metrics (Across Folds) ====\")\n",
        "    for model_name in classwise_scores:\n",
        "        print(f\"\\nModel: {model_name}\")\n",
        "        for cls in label_names:\n",
        "            print(f\"\\nClass: {cls}\")\n",
        "            for metric in [\"accuracy\", \"precision\", \"recall\", \"f1\"]:\n",
        "                scores = classwise_scores[model_name][cls][metric]\n",
        "                if scores:\n",
        "                    print(f\"{metric.capitalize()}: {np.mean(scores):.4f}\")\n",
        "\n",
        "\n",
        "# Execute\n",
        "train_all_models_foldwise()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import streamlit as st\n",
        "import torch\n",
        "import joblib\n",
        "import numpy as np\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Load label encoder\n",
        "le = joblib.load(\"label_encoder.pkl\")\n",
        "label_names = le.classes_\n",
        "\n",
        "# Constants\n",
        "MODEL_NAME = \"bert-base-uncased\"\n",
        "MODEL_PATH = \"models/bert-base-uncased_fold1.pt\"  # adjust if using a different fold\n",
        "MAX_LENGTH = 128\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(MODEL_NAME)\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_NAME, num_labels=len(label_names))\n",
        "model.load_state_dict(torch.load(MODEL_PATH, map_location=torch.device('cpu')))\n",
        "model.eval()\n",
        "\n",
        "# App title and description\n",
        "st.title(\"Question Topic Classifier\")\n",
        "st.markdown(\"This app uses a fine-tuned **BERT** model to classify questions into one of the trained topics.\")\n",
        "\n",
        "# Sidebar info\n",
        "st.sidebar.markdown(\"### Model Info\")\n",
        "st.sidebar.markdown(\"**Model:** `bert-base-uncased`\")\n",
        "st.sidebar.markdown(\"**Accuracy:** ~91% (best performer in benchmark)\")\n",
        "st.sidebar.markdown(\"**Fold:** 1 (from 5-fold CV)\")\n",
        "\n",
        "# Text input\n",
        "user_input = st.text_area(\"Enter your question:\", height=150)\n",
        "\n",
        "if st.button(\"Classify\"):\n",
        "    if not user_input.strip():\n",
        "        st.warning(\"Please enter a valid question.\")\n",
        "    else:\n",
        "        # Preprocess input\n",
        "        inputs = tokenizer(user_input, return_tensors=\"pt\", padding=True, truncation=True, max_length=MAX_LENGTH)\n",
        "\n",
        "        # Inference\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            logits = outputs.logits\n",
        "            probs = torch.nn.functional.softmax(logits, dim=1)\n",
        "            pred_idx = torch.argmax(probs, dim=1).item()\n",
        "            confidence = torch.max(probs).item()\n",
        "            predicted_label = label_names[pred_idx]\n",
        "\n",
        "        # Display result\n",
        "        st.success(f\"**Predicted Topic:** `{predicted_label}`\")\n",
        "        st.info(f\"**Confidence:** `{confidence*100:.2f}%`\")\n",
        "\n",
        "        # Optional: Show top-k probabilities\n",
        "        st.subheader(\"Prediction Probabilities:\")\n",
        "        top_probs = probs.numpy().flatten()\n",
        "        prob_table = {label_names[i]: f\"{top_probs[i]*100:.2f}%\" for i in range(len(label_names))}\n",
        "        st.json(prob_table)\n"
      ],
      "metadata": {
        "id": "bTRxj3u7x4uU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Quantum Enhanced Transformer Model (BERT - Highest accuracy)**"
      ],
      "metadata": {
        "id": "hvkuY4hOybBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers pennylane scikit-learn torch --quiet"
      ],
      "metadata": {
        "id": "mN-OJgx4x4q8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets --quiet"
      ],
      "metadata": {
        "id": "yL6aJnZsx4oY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from tqdm import tqdm\n",
        "from datasets import Dataset\n",
        "import pennylane as qml\n",
        "import matplotlib.pyplot as plt\n",
        "import gc"
      ],
      "metadata": {
        "id": "G8G8fp_9x4l-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# üì¶ Load Dataset and Generate BERT Embeddings\n",
        "# ===============================\n",
        "dataset = pd.read_csv(\"questions-data-new.csv\")\n",
        "questions = dataset['question'].tolist()\n",
        "labels = dataset['topic'].tolist()\n",
        "\n",
        "# Load BERT\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "model = BertModel.from_pretrained('bert-base-uncased')\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device).eval()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "def get_bert_embeddings(text_list, batch_size=16):\n",
        "    embeddings = []\n",
        "    for i in range(0, len(text_list), batch_size):\n",
        "        batch = text_list[i:i+batch_size]\n",
        "        inputs = tokenizer(batch, padding=True, truncation=True, return_tensors=\"pt\", max_length=128)\n",
        "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs)\n",
        "            cls_embeddings = outputs.last_hidden_state[:, 0, :].cpu().numpy()\n",
        "            embeddings.append(cls_embeddings)\n",
        "        gc.collect()\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "bert_embeddings = get_bert_embeddings(questions)\n",
        "print(\"Shape of BERT embeddings:\", bert_embeddings.shape)"
      ],
      "metadata": {
        "id": "8JYCY87Ax4jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ===============================\n",
        "# üîÑ PCA + LDA Reduction\n",
        "# ===============================\n",
        "pca = PCA(n_components=0.95)\n",
        "bert_pca = pca.fit_transform(bert_embeddings)\n",
        "print(\"Shape after PCA:\", bert_pca.shape)\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "encoded_labels = label_encoder.fit_transform(labels)\n"
      ],
      "metadata": {
        "id": "O918AUNCx4gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ‚öõÔ∏è Quantum Encoding (Amplitude)\n",
        "# ===============================\n",
        "def normalize(vec):\n",
        "    norm = np.linalg.norm(vec)\n",
        "    return vec / norm if norm != 0 else vec\n",
        "\n",
        "def direct_amplitude_encoding(x, num_qubits):\n",
        "    vec_len = 2 ** num_qubits\n",
        "    if len(x) < vec_len:\n",
        "        x = np.pad(x, (0, vec_len - len(x)), 'constant')\n",
        "    else:\n",
        "        x = x[:vec_len]\n",
        "    x = normalize(x)\n",
        "    dev = qml.device('default.qubit', wires=num_qubits)\n",
        "\n",
        "    @qml.qnode(dev)\n",
        "    def circuit():\n",
        "        qml.AmplitudeEmbedding(x, wires=range(num_qubits), normalize=True)\n",
        "        return qml.state()\n",
        "\n",
        "    state = circuit()\n",
        "    features =  np.concatenate([state.real, state.imag])\n",
        "    return features\n",
        "\n",
        "# ===============================\n",
        "# **2. Segmented Amplitude Encoding**\n",
        "# ===============================\n",
        "\n",
        "def next_power_of_2(n):\n",
        "    return 1 if n == 0 else 2**(n - 1).bit_length()\n",
        "\n",
        "def segmented_amplitude_encoding(x, num_segments=3, target_len=512):\n",
        "    # Divide the vector into segments\n",
        "    segment_length = len(x) // num_segments\n",
        "    segments = []\n",
        "\n",
        "    for i in range(num_segments):\n",
        "        segment = x[i * segment_length : (i + 1) * segment_length]\n",
        "\n",
        "        # Normalize the segment\n",
        "        segment = normalize(segment)\n",
        "\n",
        "        # Padding to the next power of 2 (to match qubit count)\n",
        "        seg_target_len = next_power_of_2(len(segment))\n",
        "        if len(segment) < seg_target_len:\n",
        "            segment = np.pad(segment, (0, seg_target_len - len(segment)), mode='constant')\n",
        "\n",
        "        segments.append(segment)\n",
        "\n",
        "    # Flatten the segments for input to the quantum device\n",
        "    flattened_segments = np.concatenate(segments)\n",
        "\n",
        "    # Ensure that the flattened vector's length matches a power of 2 for qubit embedding\n",
        "    target_len = 512  # You want the final length to be 512 (or other power of 2)\n",
        "    if len(flattened_segments) < target_len:\n",
        "        flattened_segments = np.pad(flattened_segments, (0, target_len - len(flattened_segments)), mode='constant')\n",
        "    elif len(flattened_segments) > target_len:\n",
        "        flattened_segments = flattened_segments[:target_len]\n",
        "\n",
        "    # Number of qubits needed for the target length (512)\n",
        "    num_qubits = int(np.log2(len(flattened_segments)))\n",
        "    dev = qml.device('default.qubit', wires=num_qubits)\n",
        "\n",
        "    @qml.qnode(dev)\n",
        "    def circuit():\n",
        "        # Encoding the segments into the quantum state\n",
        "        qml.AmplitudeEmbedding(flattened_segments, wires=range(num_qubits), normalize=True)\n",
        "        return qml.state()\n",
        "\n",
        "    return circuit()"
      ],
      "metadata": {
        "id": "f30CDe7dx4eQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "\n",
        "class FCNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=1024, num_classes=8):\n",
        "        super(FCNN, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),  # Increased Dropout\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.BatchNorm1d(hidden_dim),\n",
        "            nn.LeakyReLU(),\n",
        "            nn.Dropout(0.5),  # Increased Dropout\n",
        "            nn.Linear(hidden_dim, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# ===============================\n",
        "# Shared Training Function with K-Fold\n",
        "# ===============================\n",
        "def train_eval_model_kfold(embedded_states, encoded_labels, name, epochs, batch_size, patience, k_folds=5):\n",
        "    kf = KFold(n_splits=k_folds, shuffle=True, random_state=42)\n",
        "    fold_results = []\n",
        "\n",
        "    # K-Fold Cross Validation\n",
        "    for fold, (train_idx, val_idx) in enumerate(kf.split(embedded_states)):\n",
        "        print(f\"\\nTraining Fold {fold+1}/{k_folds}...\")\n",
        "\n",
        "        # Splitting data into training and validation sets\n",
        "        X_train, X_val = embedded_states[train_idx], embedded_states[val_idx]\n",
        "        y_train, y_val = encoded_labels[train_idx], encoded_labels[val_idx]\n",
        "\n",
        "        # Tensors\n",
        "        X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "        y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
        "        X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "        y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
        "\n",
        "        train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        input_dim = X_train_tensor.shape[1]\n",
        "        num_classes = len(np.unique(encoded_labels))\n",
        "        model = FCNN(input_dim=input_dim, num_classes=num_classes)\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "        # Optimizer with L2 Regularization (weight_decay)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.0005, weight_decay=1e-5)\n",
        "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)\n",
        "\n",
        "        best_val_acc = 0\n",
        "        patience_counter = 0\n",
        "        best_model_state = None\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            model.train()\n",
        "            for X_batch, y_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(X_batch)\n",
        "                loss = criterion(outputs, y_batch)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            model.eval()\n",
        "            with torch.no_grad():\n",
        "                val_outputs = model(X_val_tensor)\n",
        "                val_preds = val_outputs.argmax(dim=1).numpy()\n",
        "                val_acc = accuracy_score(y_val, val_preds)\n",
        "\n",
        "            print(f\"Epoch {epoch+1}/{epochs}, Fold {fold+1}, Validation Accuracy: {val_acc:.4f}\")\n",
        "            scheduler.step(val_acc)\n",
        "\n",
        "            if val_acc > best_val_acc:\n",
        "                best_val_acc = val_acc\n",
        "                best_model_state = model.state_dict()\n",
        "                patience_counter = 0\n",
        "            else:\n",
        "                patience_counter += 1\n",
        "\n",
        "            if patience_counter >= patience:\n",
        "                print(\"‚èπÔ∏è Early stopping triggered.\")\n",
        "                break\n",
        "\n",
        "        model.load_state_dict(best_model_state)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_outputs = model(X_val_tensor)\n",
        "            val_preds = val_outputs.argmax(dim=1).numpy()\n",
        "            val_acc = accuracy_score(y_val, val_preds)\n",
        "            fold_results.append(val_acc)\n",
        "\n",
        "    # After K-Fold training, compute the average accuracy across folds\n",
        "    avg_accuracy = np.mean(fold_results)\n",
        "    print(f\"\\n‚úÖ Average Validation Accuracy across {k_folds} folds: {avg_accuracy:.4f}\")\n",
        "\n",
        "    # Final Test Evaluation on all data\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        test_outputs = model(torch.tensor(embedded_states, dtype=torch.float32))\n",
        "        test_preds = test_outputs.argmax(dim=1).numpy()\n",
        "        test_acc = accuracy_score(encoded_labels, test_preds)\n",
        "\n",
        "    print(f\"üéØ Final Test Accuracy: {test_acc:.4f}\")\n",
        "    print(classification_report(encoded_labels, test_preds))\n",
        "\n",
        "    return name, test_acc\n",
        "\n",
        "\n",
        "def evaluate_direct_amplitude(X, y, num_qubits):\n",
        "    print(f\"\\nüöÄ Evaluating Direct Amplitude Encoding with {num_qubits} qubits...\")\n",
        "    embedded_states = []\n",
        "    for vec in tqdm(X, desc=f\"Embedding {num_qubits}q\"):\n",
        "        emb = direct_amplitude_encoding(vec, num_qubits=num_qubits)\n",
        "        embedded_states.append(emb)\n",
        "    embedded_states = np.array(embedded_states)\n",
        "    name = f\"Amplitude-{num_qubits}q\"\n",
        "    return train_eval_model_kfold(embedded_states, y, name, epochs=30, batch_size=32, patience=5, k_folds=5)\n",
        "\n",
        "def evaluate_segmented_amplitude(X, y, num_segments):\n",
        "    print(f\"\\nüöÄ Evaluating Segmented Amplitude Encoding with {num_segments} segments...\")\n",
        "    embedded_states = []\n",
        "    for vec in tqdm(X, desc=f\"Segmenting {num_segments}\"):\n",
        "        emb = segmented_amplitude_encoding(vec, num_segments=num_segments)\n",
        "        embedded_states.append(np.concatenate([emb.real, emb.imag]))\n",
        "    embedded_states = np.array(embedded_states)\n",
        "    name = f\"Segmented-{num_segments}s\"\n",
        "    return train_eval_model_kfold(embedded_states, y, name, epochs=30, batch_size=32, patience=5, k_folds=5)\n",
        "\n",
        "# ===============================\n",
        "# Quantum Evaluation for Different Qubits with K-Fold Cross Validation\n",
        "# ===============================\n",
        "results = {}\n",
        "\n",
        "for nq in [6, 7, 8, 9]:\n",
        "    name, acc = evaluate_direct_amplitude(X=bert_pca, y=encoded_labels, num_qubits=nq)\n",
        "    results[name] = acc\n",
        "\n",
        "# Evaluate using segmented amplitude encoding for different segment counts\n",
        "for segs in [3, 6, 9]:\n",
        "    name, acc = evaluate_segmented_amplitude(X=bert_pca, y=encoded_labels, num_segments=segs)\n",
        "    results[name] = acc\n",
        "\n",
        "# Final result print\n",
        "print(\"\\nüìä Final Results:\")\n",
        "for name, acc in results.items():\n",
        "    print(f\"{name}: {acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "1t5rAGPoz1vM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}